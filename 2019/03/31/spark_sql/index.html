<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Spark SQL, 谢舟，博客，读书总结，Python，Matlab，R，Java">
    <meta name="description" content="读书总结,Python,Matlab,R,Java的自己实践中的经验">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
	<meta name="baidu-site-verification" content="2VN5bX64Cz" />
    <title>Spark SQL | 谢舟的博客</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/css/my.css">
	<link rel="stylesheet" type="text/css" href="/libs/prism/prism.css">
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/jquery/jquery.min.js"></script>
    <script src="/libs/prism/prism.js"></script>
<meta name="generator" content="Hexo 8.1.1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="谢舟的博客" type="application/atom+xml">
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
					<div>
						
						<img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/logo.png" class="logo-img" alt="LOGO">
						
						<span class="logo-span">谢舟的博客</span>
					</div>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">谢舟的博客</div>
        <div class="logo-desc">
            
            读书总结,Python,Matlab,R,Java的自己实践中的经验
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>

        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/featureimages/11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Spark SQL</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">
                                <span class="chip bg-color">大数据</span>
                            </a>
                        
                            <a href="/tags/Spark/">
                                <span class="chip bg-color">Spark</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                大数据
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-03-31
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    4.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    22 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>Spark SQL 类似于Hive</p>
<h3 id="一、Spark-SQL-基础">一、Spark SQL 基础</h3>
<h4 id="1、什么是Spark-SQL">1、什么是Spark SQL</h4>
<p>Spark SQL is Apache Spark’s module for working with structured data.<br>
Spark SQL 是spark 的一个模块。来处理 结构化 的数据<br>
不能处理非结构化的数据</p>
<p>特点：</p>
<p><strong>1）容易集成</strong></p>
<p>不需要单独安装</p>
<p><strong>2）统一的数据访问方式</strong></p>
<p>结构化数据的类型：JDBC JSon Hive parquer文件 都可以作为Spark SQL 的数据源<br>
对接多种数据源，且使用方式类似</p>
<p><strong>3）完全兼容hive</strong></p>
<p>把Hive中的数据，读取到Spark SQL中运行</p>
<p><strong>4）支持标准的数据连接</strong></p>
<p>JDBC</p>
<h4 id="2、为什么学习Spark-SQL">2、为什么学习Spark SQL</h4>
<p>执行效率比Hive高</p>
<p>hive 2.x 执行引擎可以使用 Spark</p>
<h4 id="3、核心概念：表（DataFrame-DataSet）">3、核心概念：表（DataFrame DataSet）</h4>
<p>mysql中的表：表结构、数据<br>
DataFrame：Schema、RDD（数据）</p>
<p>DataSet 在spark1.6以后，对DataFrame做了一个封装</p>
<h4 id="4、创建DataFrame">4、创建DataFrame</h4>
<p>（*）测试数据：员工表、部门表<br>
第一种方式：使用case class<br>
<strong>1）定义Schema</strong><br>
样本类来定义Schema</p>
<p>case class 特点：<br>
可以支持模式匹配，使用case class建立表结构</p>
<p>7521, WARD, SALESMAN,7698, 1981/2/22, 1250, 500, 30</p>
<p>case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredate:String,sal:Int,comm:Int,deptno:Int)</p>
<p><strong>2）读取文件</strong><br>
val lines = sc.textFile(“/root/hd/tmp_files/emp.csv”).map(_.split(“,”))</p>
<p><strong>3）把每行数据，映射到Emp上</strong><br>
val allEmp = lines.map(x =&gt; Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))</p>
<p><strong>4）生成DataFrame</strong><br>
val df1 = allEmp.toDF</p>
<p>df1.show</p>
<p><strong>第二种方式 使用Spark Session</strong><br>
（1）什么是Spark Session<br>
<strong>Spark session available as ‘spark’.</strong><br>
2.0以后引入的统一访问方式。可以访问所有的Spark组件</p>
<p>def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame</p>
<p>（2）使用StructType来创建Schema</p>
<p>val struct =<br>
StructType(<br>
StructField(“a”, IntegerType, true) ::<br>
StructField(“b”, LongType, false) ::<br>
StructField(“c”, BooleanType, false) :: Nil)</p>
<p>case class Emp(<br>
empno:Int,<br>
ename:String,<br>
job:String,<br>
mgr:Int,<br>
hiredate:String,<br>
sal:Int,<br>
comm:Int,<br>
deptno:Int)</p>
<p>—————–分割———————-<br>
import org.apache.spark.sql.types._</p>
<p>val myschema = StructType(<br>
List(<br>
StructField(“empno”,DataTypes.IntegerType),<br>
StructField(“ename”,DataTypes.StringType),<br>
StructField(“job”,DataTypes.StringType),<br>
StructField(“mgr”,DataTypes.IntegerType),<br>
StructField(“hiredate”,DataTypes.StringType),<br>
StructField(“sal”,DataTypes.IntegerType),<br>
StructField(“comm”,DataTypes.IntegerType),<br>
StructField(“deptno”,DataTypes.IntegerType)<br>
))</p>
<p>准备数据 RDD[Row]<br>
import org.apache.spark.sql.Row</p>
<p>val allEmp = lines.map(x =&gt; Row(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))</p>
<p>val df2 = spark.createDataFrame(allEmp,myschema)</p>
<p>df2.show</p>
<p><strong>第三种方式</strong></p>
<p>直接读取一个带格式的文件<br>
在/root/hd/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources有现成的json代码</p>
<p>val df3 = spark.read 读文件，默认是Parquet文件<br>
val df3 = spark.read.json(“/uroot/hd/tmp_files/people.json”)</p>
<p>df3.show</p>
<p>val df4 = spark.read.format(“json”).load(“/root/hd/tmp_files/people.json”)</p>
<p>df4.show</p>
<h4 id="5、操作DataFrame">5、操作DataFrame</h4>
<p><strong>1）DSL语句</strong><br>
mybatis Hibernate</p>
<p>df1.printSchema</p>
<p>df1.select(“ename”,“sal”).show</p>
<p>df1.select($“ename”,$“sal”,$“sal”+100).show<br>
$”sal” 可以看做是一个变量</p>
<p>查询薪水大于2000的员工<br>
df1.filter($”sal” &gt; 2000).show</p>
<p>求每个部门的员工人数<br>
df1.groupBy($”deptno”).count.show</p>
<p>相当于select deptno,count(1) from emp group by deptno</p>
<p><strong>2）SQL语句</strong></p>
<p>注意：不能直接执行SQL，需要生成一个视图，再执行sql</p>
<p>scala&gt; df1.create<br>
createGlobalTempView createOrReplaceTempView createTempView</p>
<p>一般用到 createOrReplaceTempView createTempView<br>
视图：类似于表，但不保存数据</p>
<p>df1.createOrReplaceTempView(“emp”)</p>
<p>操作：<br>
spark.sql(“select * from emp”).show</p>
<p>查询薪水大于2000的员工<br>
spark.sql(“select * from emp where sal &gt; 2000”).show</p>
<p>求每个部门的员工人数<br>
spark.sql(“select deptno,count(1) from emp group by deptno”).show</p>
<p><strong>3）多表查询</strong></p>
<p>10,ACCOUNTING,NEW YORK</p>
<p>case class Dept(deptno:Int,dname:String,loc:String)<br>
val lines = sc.textFile(“/root/hd/tmp_files/dept.csv”).map(_.split(“,”))<br>
val allDept = lines.map(x=&gt;Dept(x(0).toInt,x(1),x(2)))</p>
<p>df5.createOrReplaceTempView(“dept”)</p>
<p>spark.sql(“select dname,ename from emp,dept where emp.deptno=dept.deptno”).show</p>
<h4 id="6、操作DataSet">6、操作DataSet</h4>
<p>Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点（强类型，可以使用功能强大的lambda）以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)</p>
<p>Dataset跟DataFrame类似，是一套新的接口，是高级的Dataframe</p>
<p>举例：</p>
<p><strong>1）创建DataSet</strong></p>
<p>（1）使用序列来创建DataSet<br>
定义一个case class<br>
case class MyData(a:Int,b:String)</p>
<p>生成序列，并创建DataSet<br>
val ds = Seq(MyData(1,”Tom”),MyData(2,”Merry”)).toDS</p>
<p>.toDS 生成DataSet</p>
<p>ds.show</p>
<p>（2）使用JSON数据来创建DataSet</p>
<p>定义case class<br>
case class Person(name:String,age:BigInt)</p>
<p>通过Json数据来生成DataFrame<br>
val df = spark.read.format(“json”).load(“/root/hd/tmp_files/people.json”)</p>
<p>将DataFrame转换成DataSet<br>
<a target="_blank" rel="noopener" href="http://df.as">df.as</a>[Person].show</p>
<p><a target="_blank" rel="noopener" href="http://df.as">df.as</a>[Person] 就是一个DataSet</p>
<p>（3）使用其他数据<br>
RDD操作和DataFrame操作相结合 —&gt; DataSet</p>
<p>读取数据，创建DataSet<br>
val linesDS = spark.read.text(“/root/hd/tmp_files/test_WordCount.txt”).as[String]</p>
<p>对DataSet进行操作：<br>
val words = linesDS.flatMap(.split(” “)).filter(.length &gt; 3)</p>
<p>words.show<br>
words.collect</p>
<p>执行一个WordCount程序<br>
val result = linesDS.flatMap(.split(” “)).map((,1)).groupByKey( x =&gt; x._1).count<br>
result.show</p>
<p>排序：</p>
<pre><code class="highlight plaintext">result.orderBy($"value").show
result.orderBy($"count(1)").show</code></pre>
<p><strong>2）DataSet操作案例</strong></p>
<p>使用emp.json 生成一个DataFrame<br>
val empDF = spark.read.json(“/root/hd/tmp_files/emp.json”)</p>
<p>查询工资大于3000的员工<br>
empDF.where($”sal” &gt;= 3000).show</p>
<p>创建case class</p>
<p>case class Emp(empno:BigInt,ename:String,job:String,mgr:String,hiredate:String,sal:BigInt,comm:String,deptno:BigInt)</p>
<p>生成DataSet<br>
val empDS = <a target="_blank" rel="noopener" href="http://empDF.as">empDF.as</a>[Emp]</p>
<p>查询工资大于3000的员工<br>
empDS.filter(_.sal &gt; 3000).show</p>
<p>查询10号部门的员工<br>
empDS.filter(_.deptno == 10).show</p>
<p><strong>3）多表查询</strong></p>
<p>（1）创建部门表<br>
val deptRDD = sc.textFile(“/root/hd/tmp_files/dept.csv”).map(_.split(“,”))<br>
case class Dept(deptno:Int,dname:String,loc:String)</p>
<p>val deptDS = deptRDD.map( x=&gt; Dept(x(0).toInt,x(1),x(2))).toDS</p>
<p>（2）创建员工表<br>
case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredate:String,sal:Int,comm:Int,deptno:Int)<br>
val empRDD = sc.textFile(“/root/hd/tmp_files/emp.csv”).map(_.split(“,”))</p>
<p>7369,SMITH,CLERK,7902,1980/12/17,800,0,20<br>
val empDS = empRDD.map(x=&gt; Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt)).toDS</p>
<p>（3）执行多表查询：等值连接<br>
val result = deptDS.join(empDS,”deptno”)<br>
result.show<br>
result.printSchema</p>
<p>val result1 = deptDS.joinWith(empDS, deptDS(“deptno”) === empDS(“deptno”) )<br>
result1.show<br>
result1.printSchema</p>
<p>join 和 joinWith 区别：连接后schema不同</p>
<p>join ：将两张表展开成一张更大的表<br>
joinWith ：把两张表的数据分别做成一列，然后直接拼在一起</p>
<p><strong>4）多表连接后再筛选</strong></p>
<p>deptDS.join(empDS,”deptno”).where(“deptno == 10”).show</p>
<p>result.explain：执行计划</p>
<h4 id="7、Spark-SQL-中的视图">7、Spark SQL 中的视图</h4>
<p>视图是一个虚表，不存储数据<br>
两种类型：</p>
<p><strong>1）普通视图（本地视图）</strong></p>
<p>只在当前Session中有效createOrReplaceTempView createTempView</p>
<p><strong>2）全局视图</strong></p>
<p>createGlobalTempView<br>
在不同的Session中都有用，把全局视图创建在命名空间中：global_temp中。类似于一个库</p>
<p>scala&gt; df1.create<br>
createGlobalTempView createOrReplaceTempView createTempView</p>
<p>举例：<br>
创建一个新session，读取不到emp视图，报错<br>
df1.createOrReplaceTempView(“emp”)<br>
spark.sql(“select * from emp”).show<br>
spark.newSession.sql(“select * from emp”)</p>
<p>以下两种方式均可读到全局视图中的数据<br>
df1.createGlobalTempView(“emp1”)</p>
<p>spark.newSession.sql(“select * from global_temp.emp1”).show</p>
<p>spark.sql(“select * from global_temp.emp1”).show</p>
<h3 id="二、使用数据源">二、使用数据源</h3>
<p>在Spark SQL中，可以使用各种各样的数据源来操作。 结构化</p>
<h4 id="1、使用load函数、save函数">1、使用load函数、save函数</h4>
<p>load函数是加载数据，save是存储数据</p>
<p>注意：使用load 或 save时，默认是Parquet文件。列式存储文件</p>
<p>举例:<br>
读取 users.parquet 文件<br>
val userDF = spark.read.load(“/root/hd/tmp_files/users.parquet”)</p>
<p>userDF.printSchema<br>
userDF.show</p>
<p>val userDF = spark.read.load(“/root/hd/tmp_files/emp.json”)</p>
<p>保存parquet文件</p>
<pre><code class="highlight plaintext">userDF.select($"name",$"favorite_color").write.save("/root/hd/tmp_files/parquet")</code></pre>
<p>读取刚刚写入的文件：<br>
val userDF1 = spark.read.load(“/root/hd/tmp_files/parquet/part-00000-f9a3d6bb-d481-4fc9-abf6-5f20139f97c5.snappy.parquet”)—&gt; 不推荐</p>
<p>生产中直接读取存放的目录即可：<br>
val userDF2 = spark.read.load(“/root/hd/tmp_files/parquet”)</p>
<p>读json文件 必须format<br>
val userDF = spark.read.format(“json”).load(“/root/hd/tmp_files/emp.json”)<br>
val userDF3 = spark.read.json(“/root/hd/tmp_files/emp.json”)</p>
<p>关于<strong>save函数</strong>：</p>
<p>调用save函数的时候，可以指定存储模式，追加、覆盖等等<br>
userDF.write.save(“/root/hd/tmp_files/parquet”)</p>
<p>userDF.write.save(“/root/hd/tmp_files/parquet”)<br>
org.apache.spark.sql.AnalysisException: path file:/root/hd/tmp_files/parquet already exists.;</p>
<p>save的时候覆盖<br>
userDF.write.mode(“overwrite”).save(“/root/hd/tmp_files/parquet”)</p>
<p>将结果保存成表<br>
userDF.select($”name”).write.saveAsTable(“table1”)</p>
<p>scala&gt; userDF.select($”name”).write.saveAsTable(“table1”)</p>
<p>scala&gt; spark.sql(“select * from table1”).show<br>
+——+<br>
| name|<br>
+——+<br>
|Alyssa|<br>
| Ben|<br>
+——+</p>
<h4 id="2、Parquet文件">2、Parquet文件</h4>
<p>列式存储文件，是Spark SQL 默认的数据源<br>
就是一个普通的文件</p>
<p>举例：<br>
1）把其他文件，转换成Parquet文件<br>
调用save函数<br>
把数据读进来，再写出去，就是Parquet文件</p>
<p>val empDF = spark.read.json(“/root/hd/tmp_files/emp.json”)<br>
empDF.write.mode(“overwrite”).save(“/root/hd/tmp_files/parquet”)<br>
empDF.write.mode(“overwrite”).parquet(“/root/hd/tmp_files/parquet”)</p>
<p>val emp1 = spark.read.parquet(“/root/hd/tmp_files/parquet”)<br>
emp1.createOrReplaceTempView(“emp1”)<br>
spark.sql(“select * from emp1”)</p>
<p>2）支持Schema的合并<br>
项目开始 表结构简单 schema简单<br>
项目越来越大 schema越来越复杂</p>
<p>举例：<br>
通过RDD来创建DataFrame<br>
val df1 = sc.makeRDD(1 to 5).map( i =&gt; (i,i*2)).toDF(“single”,”double”)<br>
“single”,”double” 是表结构<br>
df1.show</p>
<p>df1.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/key=1”)</p>
<p>val df2 = sc.makeRDD(6 to 10).map( i =&gt; (i,i*3)).toDF(“single”,”triple”)<br>
df2.show<br>
df2.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/key=2”)</p>
<p>合并两个部分<br>
val df3 = spark.read.parquet(“/root/hd/tmp_files/test_table”)</p>
<p>val df3 = spark.read.option(“mergeSchema”,true).parquet(“/root/hd/tmp_files/test_table”)</p>
<p><strong>key是可以随意取名字的，两个key需要一致，不然合并会报错</strong></p>
<p>通过RDD来创建DataFrame<br>
val df1 = sc.makeRDD(1 to 5).map( i =&gt; (i,i*2)).toDF(“single”,”double”)<br>
“single”,”double” 是表结构<br>
df1.show</p>
<p>df1.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/kt=1”)</p>
<p>val df2 = sc.makeRDD(6 to 10).map( i =&gt; (i,i*3)).toDF(“single”,”triple”)<br>
df2.show<br>
df2.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/kt=2”)</p>
<p>合并两个部分<br>
val df3 = spark.read.parquet(“/root/hd/tmp_files/test_table”)</p>
<p>val df3 = spark.read.option(“mergeSchema”,true).parquet(“/root/hd/tmp_files/test_table”)</p>
<h4 id="3、json文件">3、json文件</h4>
<p>读取Json文件，生成DataFrame<br>
val peopleDF = spark.read.json(“/root/hd/tmp_files/people.json”)</p>
<p>peopleDF.printSchema</p>
<p>peopleDF.createOrReplaceTempView(“peopleView”)</p>
<p>spark.sql(“select * from peopleView”).show</p>
<p>Spark SQL 支持统一的访问接口。对于不同的数据源，读取进来，生成DataFrame后，操作完全一样</p>
<h4 id="4、JDBC">4、JDBC</h4>
<p>使用JDBC操作关系型数据库，加载到Spark中进行分析和处理</p>
<p>方式一：</p>
<pre><code class="highlight plaintext">./spark-shell --master spark://hsiehchou121:7077 --jars /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --driver-class-path /root/hd/tmp_files/mysql-connector-java-8.0.12.jar</code></pre>
<pre><code class="highlight plaintext">val mysqlDF = spark.read.format("jdbc")
.option("url","jdbc:mysql://192.168.116.1/company?serverTimezone=UTC&amp;characterEncoding=utf-8")
.option("driver","com.mysql.cj.jdbc.Driver")
.option("user","root")
.option("password","123456")
.option("dbtable","emp").load</code></pre>
<pre><code class="highlight plaintext">val mysqlDF = spark.read.format("jdbc").option("url","jdbc:mysql://192.168.116.1/company?serverTimezone=UTC&amp;characterEncoding=utf-8").option("driver","com.mysql.cj.jdbc.Driver").option("user","root").option("password","123456").option("dbtable","emp").load
mysqlDF.show</code></pre>
<p><strong>问题解决</strong></p>
<p>如果遇到下面问题，就是你本机的mysql数据库没有权限给你虚拟机访问<br>
java.sql.SQLException: null, message from server: “Host ‘hsiehchou121’ is not allowed to connect to this MySQL server”</p>
<p><strong>解决方案</strong></p>
<p>1）进入你本机的数据库<br>
mysql -u root -p<br>
2）use mysql;<br>
3）修改root用户前面的Host，改为%，意思是全部IP都能访问<br>
4）flush privileges;</p>
<p><strong>方式二</strong><br>
定义一个Properties类<br>
import java.util.Properties<br>
val mysqlProps = new Properties()<br>
mysqlProps.setProperty(“driver”,“com.mysql.cj.jdbc.Driver”)<br>
mysqlProps.setProperty(“user”,“root”)<br>
mysqlProps.setProperty(“password”,“123456”)</p>
<p>val mysqlDF1 = spark.read.jdbc(“jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8”,“emp”,mysqlProps)</p>
<p>mysqlDF1.show</p>
<h4 id="5、使用Hive">5、使用Hive</h4>
<p>比较常见<br>
（<em>）spark SQL 完全兼容hive<br>
（</em>）需要进行配置<br>
拷贝一下文件到spark/conf目录下：<br>
Hive 配置文件： hive-site.xml<br>
Hadoop 配置文件：core-site.xml hdfs-site.xml</p>
<p>配置好后，重启spark</p>
<p><strong>在hive的lib下和spark的jars下面增加mysql-connector-java-8.0.12.jar这边连接数据库的jar包</strong></p>
<p>启动Hadoop ：<a target="_blank" rel="noopener" href="http://start-all.sh">start-all.sh</a><br>
启动 hive：</p>
<pre><code class="highlight plaintext">hsiehchou121
cd hive/bin/
./hive --service metastore

hsiehchou122
cd hive/bin
./hive</code></pre>
<p><strong>hsiehchou121启动问题</strong></p>
<p>java.sql.SQLSyntaxErrorException: Table ‘hive.version’ doesn’t exist<br>
解决：去mysql数据库中的hive库下面创建version表<br>
这里需要给本地的hive库创建下hive所必须用的表</p>
<p>我们去/root/hd/hive/scripts/metastore/upgrade/mysql这里面找到hive-schema-1.2.0.mysql.sql，将里面的sql语句在hive库中执行</p>
<p>hive-txn-schema-0.14.0.mysql.sql，这个也做好执行下，用于事务管理</p>
<p><strong>显示当前所在库名字</strong></p>
<p>set hive.cli.print.current.db=true;</p>
<p>j将emp.csv上传到hdfs中的/tmp_files/下面<br>
hdfs dfs -put emp.csv /tmp_files</p>
<p>在hive中创建emp_default表</p>
<pre><code class="highlight plaintext">hive (default)&gt; create table emp(empno int,ename string,job string,mgr int,hiredate string,sal int,comm int,deptno int)
              &gt; row format
              &gt; delimited fields
              &gt; terminated by ",";
hive (default)&gt; load data inpath '/tmp_files/emp.csv' into table emp;
Time taken: 1.894 seconds
hive (default)&gt; show tables;
hive (default)&gt; select * from emp;</code></pre>
<p>hdfs dfs -put /root/hd/tmp_files/emp.csv /tmp_files</p>
<pre><code class="highlight plaintext">[root@hsiehchou121 bin]# ./spark-shell --master spark://hsiehchou121:7077</code></pre>
<p>启动Spatk时，如果出现如下错误<br>
java.sql.SQLSyntaxErrorException: Table ‘hive.partitions’ doesn’t exist<br>
在MySQL数据库里面创建partitions表</p>
<p>scala&gt; spark.sql(“select * from emp_default”).show<br>
scala&gt; spark.sql(“select * from default.emp_default”).show</p>
<p>spark.sql(“create table company.emp_4(empno Int,ename String,job String,mgr String,hiredate String,sal Int,comm String,deptno Int)row format delimited fields terminated by ‘,’”)<br>
spark.sql(“load data local inpath ‘/root/hd/tmp_files/emp.csv’ overwrite into table company.emp_4”)</p>
<h3 id="三、在IDE中开发Spark-SQL">三、在IDE中开发Spark SQL</h3>
<h4 id="1、创建DataFrame-StructType方式">1、创建DataFrame StructType方式</h4>
<pre><code class="highlight plaintext">package day4

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.Row
import org.apache.log4j.Logger
import org.apache.log4j.Level

/**
 * 创建DataFrame StructType方式
 */
object Demo1 {
  def main(args: Array[String]): Unit = {

    //减少Info日志的打印
    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)

    //创建Spark Session对象
    val spark = SparkSession.builder().master("local").appName("Demo1").getOrCreate()

    //从指定的地址创建RDD对象
    val personRDD = spark.sparkContext.textFile("H:\\other\\students.txt").map(_.split("\t"))

    //通过StructType方式指定Schema
    val schema = StructType(
      List(
        StructField("id", IntegerType),
        StructField("name", StringType),
        StructField("age", IntegerType)))

    //将RDD映射到rowRDD上，映射到Schema上
    val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt,p(1),p(2).toInt))
    val personDataFrame = spark.createDataFrame(rowRDD, schema)

    //注册视图
    personDataFrame.createOrReplaceTempView("t_person")

    //执行SQL语句  desc降序   asc 升序
    val df = spark.sql("select * from t_person order by age desc")

    df.show

    spark.stop()
  }
}</code></pre>
<h4 id="2、使用case-class来创建DataFrame">2、使用case class来创建DataFrame</h4>
<pre><code class="highlight plaintext">package day4

import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.sql.SparkSession

/**
 * 使用case class来创建DataFrame
 */
object Demo2 {
  def main(args: Array[String]): Unit = {

    //减少Info日志的打印
    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)

    //创建Spark Session对象
    val spark = SparkSession.builder().master("local").appName("Demo1").getOrCreate()

    //从指定的地址创建RDD对象
    val lineRDD = spark.sparkContext.textFile("H:\\other\\students.txt").map(_.split("\t"))

    //把数据与case class做匹配
    val studentRDD = lineRDD.map(x =&gt; Student(x(0).toInt,x(1),x(2).toInt))

    //生成DataFrame
    import spark.sqlContext.implicits._
    val studentDF = studentRDD.toDF()

    //注册视图,执行SQL
    studentDF.createOrReplaceTempView("student")

    spark.sql("select * from student").show

    spark.stop()
  }
}

//定义case class
case class Student(stuId:Int, stuName:String, stuAge:Int)</code></pre>
<h4 id="3、写入MySQL">3、写入MySQL</h4>
<pre><code class="highlight plaintext">package day4

import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import java.util.Properties

/**
 * 写入mysql
 */
object Demo3 {
  def main(args: Array[String]): Unit = {

    //减少Info日志的打印
    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)

    //创建Spark Session对象
    val spark = SparkSession.builder().master("local").appName("Demo1").getOrCreate()

    //从指定的地址创建RDD对象
    val lineRDD = spark.sparkContext.textFile("H:\\other\\students.txt").map(_.split("\t"))

    //通过StructType方式指定Schema
    val schema = StructType(
      List(
        StructField("personID", IntegerType),
        StructField("personName", StringType),
        StructField("personAge", IntegerType)))

    //将RDD映射到rowRDD上，映射到Schema上
    val rowRDD = lineRDD.map(p =&gt; Row(p(0).toInt,p(1),p(2).toInt))
    val personDataFrame = spark.createDataFrame(rowRDD, schema)

    personDataFrame.createOrReplaceTempView("myperson")

    val result = spark.sql("select * from myperson")

    result.show

    //把结果存入mysql中
    val props = new Properties()
    props.setProperty("user", "root")
    props.setProperty("password", "123456")
    props.setProperty("driver", "com.mysql.cj.jdbc.Driver")

    result.write.mode("append").jdbc("jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8", "student", props)

    spark.stop()
  }
}</code></pre>
<h4 id="4、使用Spark-SQL-读取Hive中的数据，将计算结果存入MySQL">4、使用Spark SQL 读取Hive中的数据，将计算结果存入MySQL</h4>
<pre><code class="highlight plaintext">package day4

import org.apache.spark.sql.SparkSession
import java.util.Properties

/**
 * 使用Spark SQL 读取Hive中的数据，将计算结果存入mysql
 */
object Demo4 {
  def main(args: Array[String]): Unit = {

    //创建SparkSession
    val spark = SparkSession.builder().appName("Demo4").enableHiveSupport().getOrCreate()

    //执行SQL
    val result = spark.sql("select deptno,count(1) from company.emp group by deptno")

    //将结果保存到mysql中
     val props = new Properties()
    props.setProperty("user", "root")
    props.setProperty("password", "123456")
    props.setProperty("driver", "com.mysql.cj.jdbc.Driver")

    result.write.jdbc("jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8", "emp_stat", props)

    spark.stop()
  } 
}</code></pre>
<p><strong>提交任务</strong></p>
<pre><code class="highlight plaintext">[root@hsiehchou121 bin]# ./spark-submit --master spark://hsiehchou121:7077 --jars /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --driver-class-path /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --class day4.Demo4 /root/hd/tmp_files/Demo4.jar</code></pre>
<h3 id="四、性能优化">四、性能优化</h3>
<p>与RDD类似</p>
<h4 id="1、把内存中缓存表的数据">1、把内存中缓存表的数据</h4>
<p>直接读取内存的值，来提高性能</p>
<p>RDD中如何缓存：<br>
rdd.cache 或者 rdd.persist</p>
<p>在Spark SQL中，使用SparkSession.sqlContext.cacheTable</p>
<p>spark中所有context对象<br>
1）sparkContext ： SparkCore<br>
2）sql Context ： SparkSQL<br>
3）Streaming Context ：SparkStreaming</p>
<p>统一起来：SparkSession</p>
<p>操作mysql，启动spark shell 时，需要：<br>
./spark-shell <code>--master</code> spark://hsiehchou121:7077 <code>--jars</code> /root/hd/tmp_files/mysql-connector-java-8.0.12.jar <code>--driver-class-path</code> /root/hd/tmp_files/mysql-connector-java-8.0.12.jar</p>
<p>val mysqlDF = spark.read.format(“jdbc”).option(“driver”,”com.mysql.cj.jdbc.Driver”).option(“url”,”jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8”).option(“user”,”root”).option(“password”,”123456”).option(“dbtable”,”emp”).load</p>
<p>mysqlDF.show<br>
mysqlDF.createOrReplaceTempView(“emp”)</p>
<p>spark.sqlContext.cacheTable(“emp”) —-&gt; 标识这张表可以被缓存，数据还没有真正被缓存<br>
spark.sql(“select * from emp”).show —-&gt; 依然读取mysql<br>
spark.sql(“select * from emp”).show —-&gt; 从缓存中读取数据</p>
<p>spark.sqlContext.clearCache</p>
<p>清空缓存后，执行查询，会触发查询mysql数据库</p>
<h4 id="2、了解性能优化的相关参数">2、了解性能优化的相关参数</h4>
<p>将数据缓存到内存中的相关优化参数<br>
spark.sql.inMemoryColumnarStorage.compressed<br>
默认为 true<br>
Spark SQL 将会基于统计信息自动地为每一列选择一种压缩编码方式</p>
<p>spark.sql.inMemoryColumnarStorage.batchSize<br>
默认值：10000<br>
缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险</p>
<p>其他性能相关的配置选项（不过不推荐手动修改，可能在后续版本自动的自适应修改）<br>
spark.sql.files.maxPartitionBytes<br>
默认值：128 MB<br>
读取文件时单个分区可容纳的最大字节数</p>
<p>spark.sql.files.openCostInBytes<br>
默认值：4M<br>
打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)</p>
<p>spark.sql.autoBroadcastJoinThreshold<br>
默认值：10M<br>
用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了 ANALYZE TABLE COMPUTE STATISTICS noscan 命令的 Hive Metastore 表</p>
<p>spark.sql.shuffle.partitions<br>
默认值：200<br>
用于配置 join 或聚合操作混洗（shuffle）数据时使用的分区数</p>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://hsiehchou.github.io" rel="external nofollow noreferrer">谢舟</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://hsiehchou.github.io/2019/03/31/spark_sql/">https://hsiehchou.github.io/2019/03/31/spark_sql/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://hsiehchou.github.io" target="_blank">谢舟</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">
                                    <span class="chip bg-color">大数据</span>
                                </a>
                            
                                <a href="/tags/Spark/">
                                    <span class="chip bg-color">Spark</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2019/04/03/spark_streaming_ji_chu/">
                    <div class="card-image">
                        
                        
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="Spark Streaming">
                        
                        <span class="card-title">Spark Streaming</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Spark Streaming
流式计算框架，类似于Storm
常用的实时计算引擎（流式计算）
1、Apache Storm：真正的流式计算
2、Spark Streaming ：严格上来说，不是真正的流式计算（实时计算）
把连续的流式数据
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2019-04-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">
                        <span class="chip bg-color">大数据</span>
                    </a>
                    
                    <a href="/tags/Spark/">
                        <span class="chip bg-color">Spark</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/03/29/spark_core/">
                    <div class="card-image">
                        
                        
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/featureimages/6.jpg" class="responsive-img" alt="Spark Core">
                        
                        <span class="card-title">Spark Core</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Spark生态圈：
Spark Core ： RDD（弹性分布式数据集）
Spark SQL
Spark Streaming
Spark MLLib ：协同过滤，ALS，逻辑回归等等 –&gt; 机器学习
Spark Graphx ： 图计
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2019-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">
                        <span class="chip bg-color">大数据</span>
                    </a>
                    
                    <a href="/tags/Spark/">
                        <span class="chip bg-color">Spark</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('10')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 谢舟的博客<br />'
            + '文章作者: 谢舟<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者谢舟所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019-2026</span>
            <a href="https://hsiehchou.github.io" target="_blank">谢舟</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">552.6k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            
			<br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">


    <a href="mailto:babbyxie@foxmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=417952939" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 417952939" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', '');
</script>


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?1fdd6e11866c1fe7b815d69a4a4206ea";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    
    
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io" type="module"></script>
    

</body>

</html>
